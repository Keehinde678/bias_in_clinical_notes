{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc0be6bc-7469-4699-89cc-ee8795e84eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIMIC-III CLINICAL TEXT BIAS ANALYSIS\n",
      "Focus: Racial Bias in Compliance Language\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CONFIGURATION\n",
    "DATA_PATH = \"C:/Users/kehin/Downloads/mimic-iii-extracted/\"\n",
    "\n",
    "# Compliance Language Lexicon\n",
    "LEXICON = {\n",
    "    'negative_compliance': [\n",
    "        'non-compliant', 'noncompliant', 'non compliant',\n",
    "        'uncooperative', 'un-cooperative', 'resistant',\n",
    "        'refuses', 'refused', 'refusing', 'refusal',\n",
    "        'difficult', 'combative', 'aggressive', 'hostile',\n",
    "        'agitated', 'demanding', 'manipulative', \n",
    "        'drug-seeking', 'drug seeking', 'narcotic-seeking',\n",
    "        'non-adherent', 'nonadherent', 'non adherent'\n",
    "    ],\n",
    "    'positive_compliance': [\n",
    "        'compliant', 'cooperative', 'pleasant',\n",
    "        'agreeable', 'appropriate', 'follows instructions',\n",
    "        'adherent', 'complies', 'willing', 'cooperative'\n",
    "    ],\n",
    "    'neutral_descriptors': [\n",
    "        'alert', 'oriented', 'responsive',\n",
    "        'stable', 'calm', 'resting', 'comfortable'\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "print(\"MIMIC-III CLINICAL TEXT BIAS ANALYSIS\")\n",
    "print(\"Focus: Racial Bias in Compliance Language\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a90d2-158f-4318-a26e-d9723d9a1915",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d467657-b811-43e9-acf8-055acd973814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading MIMIC-III data files...\n",
      "----------------------------------------------------------------------\n",
      "✓ PATIENTS.csv: 46,520 patients\n",
      "✓ ADMISSIONS.csv: 58,976 admissions\n",
      "✓ NOTEEVENTS.csv: 2,083,180 clinical notes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "print(\"Step 1: Loading MIMIC-III data files...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Load the three main files\n",
    "patients = pd.read_csv(r\"C:\\Users\\kehin\\Downloads\\mimic-iii-extracted\\mimic-iii-clinical-database-1.4\\PATIENTS.csv.gz\")\n",
    "admissions = pd.read_csv(r\"C:\\Users\\kehin\\Downloads\\mimic-iii-extracted\\mimic-iii-clinical-database-1.4\\ADMISSIONS.csv.gz\")\n",
    "notes = pd.read_csv(r\"C:\\Users\\kehin\\Downloads\\mimic-iii-extracted\\mimic-iii-clinical-database-1.4\\NOTEEVENTS.csv.gz\")\n",
    "print(f\"✓ PATIENTS.csv: {len(patients):,} patients\")\n",
    "print(f\"✓ ADMISSIONS.csv: {len(admissions):,} admissions\")\n",
    "print(f\"✓ NOTEEVENTS.csv: {len(notes):,} clinical notes\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63d1cc72-6869-48ce-ad57-b50446794e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Merging demographic data...\n",
      "----------------------------------------------------------------------\n",
      "Merged dataset created with 59,652 rows\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Merging demographic data\n",
    "print(\"Step 3: Merging demographic data...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Filter notes to only include discharge summaries\n",
    "discharge_notes = notes[notes['CATEGORY'] == 'Discharge summary'].copy()\n",
    "\n",
    "# Merge discharge notes with admissions data\n",
    "merged = discharge_notes.merge(\n",
    "    admissions[['SUBJECT_ID', 'HADM_ID', 'ETHNICITY', 'ADMISSION_TYPE', 'ADMITTIME']], \n",
    "    on=['SUBJECT_ID', 'HADM_ID'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Merged dataset created with {len(merged):,} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47ac7c77-d88a-4ef1-9c40-5f8e397174f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 59652 discharge notes\n",
      "\n",
      "Step 1: Merging with admissions...\n",
      "After admissions merge: 59652 rows\n",
      "Columns: ['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME', 'STORETIME', 'CATEGORY', 'DESCRIPTION', 'CGID', 'ISERROR', 'TEXT', 'ETHNICITY', 'ADMISSION_TYPE', 'ADMITTIME']\n",
      "\n",
      "Step 2: Checking patients data...\n",
      "Patients columns: ['ROW_ID', 'SUBJECT_ID', 'GENDER', 'DOB', 'DOD', 'DOD_HOSP', 'DOD_SSN', 'EXPIRE_FLAG']\n",
      "First few patient SUBJECT_IDs: 0    249\n",
      "1    250\n",
      "2    251\n",
      "3    252\n",
      "4    253\n",
      "Name: SUBJECT_ID, dtype: int64\n",
      "First few merged SUBJECT_IDs: 0    22532\n",
      "1    13702\n",
      "2    13702\n",
      "3    13702\n",
      "4    26880\n",
      "Name: SUBJECT_ID, dtype: int64\n",
      "\n",
      "Step 3: Merging with patients...\n",
      "After patients merge: 59652 rows\n",
      "Columns: ['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME', 'STORETIME', 'CATEGORY', 'DESCRIPTION', 'CGID', 'ISERROR', 'TEXT', 'ETHNICITY', 'ADMISSION_TYPE', 'ADMITTIME', 'GENDER', 'DOB']\n",
      "\n",
      "✓ DOB column successfully merged!\n",
      "Non-null DOB values: 59652\n"
     ]
    }
   ],
   "source": [
    "# Start fresh - reload discharge notes\n",
    "discharge_notes = notes[notes['CATEGORY'] == 'Discharge summary'].copy()\n",
    "\n",
    "print(f\"Starting with {len(discharge_notes)} discharge notes\")\n",
    "print()\n",
    "\n",
    "# Step 1: Merge with admissions\n",
    "print(\"Step 1: Merging with admissions...\")\n",
    "merged = discharge_notes.merge(\n",
    "    admissions[['SUBJECT_ID', 'HADM_ID', 'ETHNICITY', 'ADMISSION_TYPE', 'ADMITTIME']], \n",
    "    on=['SUBJECT_ID', 'HADM_ID'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"After admissions merge: {len(merged)} rows\")\n",
    "print(f\"Columns: {merged.columns.tolist()}\")\n",
    "print()\n",
    "\n",
    "# Step 2: Check patients data before merge\n",
    "print(\"Step 2: Checking patients data...\")\n",
    "print(f\"Patients columns: {patients.columns.tolist()}\")\n",
    "print(f\"First few patient SUBJECT_IDs: {patients['SUBJECT_ID'].head()}\")\n",
    "print(f\"First few merged SUBJECT_IDs: {merged['SUBJECT_ID'].head()}\")\n",
    "print()\n",
    "\n",
    "# Step 3: Try the patients merge\n",
    "print(\"Step 3: Merging with patients...\")\n",
    "merged = merged.merge(\n",
    "    patients[['SUBJECT_ID', 'GENDER', 'DOB']], \n",
    "    on='SUBJECT_ID',\n",
    "    how='left'\n",
    ")\n",
    "print(f\"After patients merge: {len(merged)} rows\")\n",
    "print(f\"Columns: {merged.columns.tolist()}\")\n",
    "print()\n",
    "\n",
    "# Check if DOB is now present\n",
    "if 'DOB' in merged.columns:\n",
    "    print(\"✓ DOB column successfully merged!\")\n",
    "    print(f\"Non-null DOB values: {merged['DOB'].notna().sum()}\")\n",
    "else:\n",
    "    print(\"✗ DOB column NOT in merged data\")\n",
    "    print(\"This means the merge failed - investigating why...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6835cd65-00e4-4da9-b0b2-203ba2ed2760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Applying compliance language lexicon...\n",
      "----------------------------------------------------------------------\n",
      "Counting negative compliance terms...\n",
      "Counting positive compliance terms...\n",
      "Counting neutral descriptor terms...\n",
      " Lexicon analysis complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LEXICON-BASED ANALYSIS\n",
    "print(\"Step 4: Applying compliance language lexicon...\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "def count_lexicon_terms(text, term_list):\n",
    "    \"\"\"Count occurrences of terms from a list in text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    text_lower = str(text).lower()\n",
    "    count = 0\n",
    "    for term in term_list:\n",
    "        # Use word boundaries to avoid partial matches\n",
    "        pattern = r'\\b' + re.escape(term) + r'\\b'\n",
    "        count += len(re.findall(pattern, text_lower))\n",
    "    return count\n",
    "\n",
    "# Apply lexicon counting\n",
    "print(\"Counting negative compliance terms...\")\n",
    "merged['NEGATIVE_COMPLIANCE'] = merged['TEXT'].apply(\n",
    "    lambda x: count_lexicon_terms(x, LEXICON['negative_compliance'])\n",
    ")\n",
    "\n",
    "print(\"Counting positive compliance terms...\")\n",
    "merged['POSITIVE_COMPLIANCE'] = merged['TEXT'].apply(\n",
    "    lambda x: count_lexicon_terms(x, LEXICON['positive_compliance'])\n",
    ")\n",
    "\n",
    "print(\"Counting neutral descriptor terms...\")\n",
    "merged['NEUTRAL_TERMS'] = merged['TEXT'].apply(\n",
    "    lambda x: count_lexicon_terms(x, LEXICON['neutral_descriptors'])\n",
    ")\n",
    "\n",
    "# Calculate text length and term density\n",
    "merged['TEXT_LENGTH'] = merged['TEXT'].apply(lambda x: len(str(x).split()))\n",
    "merged['NEGATIVE_DENSITY'] = (merged['NEGATIVE_COMPLIANCE'] / merged['TEXT_LENGTH'] * 1000).fillna(0)\n",
    "merged['POSITIVE_DENSITY'] = (merged['POSITIVE_COMPLIANCE'] / merged['TEXT_LENGTH'] * 1000).fillna(0)\n",
    "\n",
    "print(f\" Lexicon analysis complete\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d56b70a-ffb6-4478-bd65-807b800b86ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting unique patterns\n",
      " Pattern detection complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DETECT UNIQUE PATTERNS\n",
    "print(\"Detecting unique patterns\")\n",
    "\n",
    "\n",
    "def detect_patterns(text):\n",
    "    \"\"\"Detect specific patterns in clinical text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    text_lower = str(text).lower()\n",
    "    patterns = []\n",
    "    \n",
    "    # Drug-seeking behavior\n",
    "    if re.search(r'\\bdrug.{0,10}seeking\\b', text_lower):\n",
    "        patterns.append('drug-seeking')\n",
    "    \n",
    "    # Left AMA (Against Medical Advice)\n",
    "    if 'ama' in text_lower or 'against medical advice' in text_lower:\n",
    "        patterns.append('AMA')\n",
    "    \n",
    "    # Frequent ED visits\n",
    "    if re.search(r'\\bfrequent\\s+(ed|emergency)', text_lower):\n",
    "        patterns.append('frequent-ED')\n",
    "    \n",
    "    # Non-compliance mentioned\n",
    "    if 'non-compliant' in text_lower or 'noncompliant' in text_lower:\n",
    "        patterns.append('non-compliant-documented')\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "merged['PATTERNS'] = merged['TEXT'].apply(detect_patterns)\n",
    "merged['HAS_PATTERNS'] = merged['PATTERNS'].apply(lambda x: len(x) > 0)\n",
    "\n",
    "print(f\" Pattern detection complete\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b4816c2-0ddb-4726-b032-cefa319e4905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current columns in merged dataframe:\n",
      "['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CHARTDATE', 'CHARTTIME', 'STORETIME', 'CATEGORY', 'DESCRIPTION', 'CGID', 'ISERROR', 'TEXT', 'ETHNICITY', 'ADMISSION_TYPE', 'ADMITTIME', 'GENDER', 'DOB', 'NEGATIVE_COMPLIANCE', 'POSITIVE_COMPLIANCE', 'NEUTRAL_TERMS', 'TEXT_LENGTH', 'NEGATIVE_DENSITY', 'POSITIVE_DENSITY', 'PATTERNS', 'HAS_PATTERNS']\n",
      "\n",
      "✓ ETHNICITY column exists\n",
      "\n",
      "First few ethnicity values:\n",
      "0    UNKNOWN/NOT SPECIFIED\n",
      "1                    WHITE\n",
      "2                    WHITE\n",
      "3                    WHITE\n",
      "4                    WHITE\n",
      "5                    WHITE\n",
      "6                    WHITE\n",
      "7                    WHITE\n",
      "8       HISPANIC OR LATINO\n",
      "9       HISPANIC OR LATINO\n",
      "Name: ETHNICITY, dtype: object\n",
      "\n",
      "✓ RACE_SIMPLIFIED column created\n",
      "\n",
      "Race distribution:\n",
      "RACE_SIMPLIFIED\n",
      "WHITE       42255\n",
      "UNKNOWN      6409\n",
      "BLACK        5721\n",
      "HISPANIC     2034\n",
      "OTHER        1717\n",
      "ASIAN        1516\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check what columns exist in merged\n",
    "print(\"Current columns in merged dataframe:\")\n",
    "print(merged.columns.tolist())\n",
    "print()\n",
    "\n",
    "# Check if ETHNICITY column exists\n",
    "if 'ETHNICITY' in merged.columns:\n",
    "    print(\" ETHNICITY column exists\")\n",
    "    print(\"\\nFirst few ethnicity values:\")\n",
    "    print(merged['ETHNICITY'].head(10))\n",
    "    print()\n",
    "    \n",
    "    # Now create RACE_SIMPLIFIED\n",
    "    def simplify_ethnicity(eth):\n",
    "        if pd.isna(eth):\n",
    "            return 'UNKNOWN'\n",
    "        eth = str(eth).upper()\n",
    "        if 'WHITE' in eth:\n",
    "            return 'WHITE'\n",
    "        elif 'BLACK' in eth or 'AFRICAN AMERICAN' in eth:\n",
    "            return 'BLACK'\n",
    "        elif 'HISPANIC' in eth or 'LATINO' in eth:\n",
    "            return 'HISPANIC'\n",
    "        elif 'ASIAN' in eth:\n",
    "            return 'ASIAN'\n",
    "        elif 'UNKNOWN' in eth or 'UNABLE' in eth or 'DECLINED' in eth:\n",
    "            return 'UNKNOWN'\n",
    "        else:\n",
    "            return 'OTHER'\n",
    "    \n",
    "    merged['RACE_SIMPLIFIED'] = merged['ETHNICITY'].apply(simplify_ethnicity)\n",
    "    \n",
    "    print(\"✓ RACE_SIMPLIFIED column created\")\n",
    "    print(\"\\nRace distribution:\")\n",
    "    print(merged['RACE_SIMPLIFIED'].value_counts())\n",
    "    print()\n",
    "else:\n",
    "    print(\"✗ ETHNICITY column doesn't exist\")\n",
    "    print(\"Need to go back and fix the merge with admissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3cf3ed9c-60ff-4d66-923c-73680ae8bf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics by race\n",
      "Statistics computed\n",
      "\n",
      "Summary Statistics by Race:\n",
      "                 NOTE_COUNT  NEGATIVE_COMPLIANCE_mean  \\\n",
      "RACE_SIMPLIFIED                                         \n",
      "ASIAN                  1516                     0.507   \n",
      "BLACK                  5721                     0.782   \n",
      "HISPANIC               2034                     0.550   \n",
      "OTHER                  1717                     0.580   \n",
      "UNKNOWN                6409                     0.507   \n",
      "WHITE                 42255                     0.604   \n",
      "\n",
      "                 POSITIVE_COMPLIANCE_mean  NEGATIVE_RATE_%  \n",
      "RACE_SIMPLIFIED                                             \n",
      "ASIAN                               0.554            47.79  \n",
      "BLACK                               0.592            56.93  \n",
      "HISPANIC                            0.590            48.23  \n",
      "OTHER                               0.525            52.50  \n",
      "UNKNOWN                             0.358            58.60  \n",
      "WHITE                               0.530            53.28  \n",
      "\n",
      "KEY FINDINGS:\n",
      "ASIAN:\n",
      "  • Notes: 1,516\n",
      "  • Avg negative terms per note: 0.51\n",
      "  • Avg positive terms per note: 0.55\n",
      "  • Negative rate: 47.8%\n",
      "\n",
      "BLACK:\n",
      "  • Notes: 5,721\n",
      "  • Avg negative terms per note: 0.78\n",
      "  • Avg positive terms per note: 0.59\n",
      "  • Negative rate: 56.9%\n",
      "\n",
      "HISPANIC:\n",
      "  • Notes: 2,034\n",
      "  • Avg negative terms per note: 0.55\n",
      "  • Avg positive terms per note: 0.59\n",
      "  • Negative rate: 48.2%\n",
      "\n",
      "OTHER:\n",
      "  • Notes: 1,717\n",
      "  • Avg negative terms per note: 0.58\n",
      "  • Avg positive terms per note: 0.53\n",
      "  • Negative rate: 52.5%\n",
      "\n",
      "UNKNOWN:\n",
      "  • Notes: 6,409\n",
      "  • Avg negative terms per note: 0.51\n",
      "  • Avg positive terms per note: 0.36\n",
      "  • Negative rate: 58.6%\n",
      "\n",
      "WHITE:\n",
      "  • Notes: 42,255\n",
      "  • Avg negative terms per note: 0.60\n",
      "  • Avg positive terms per note: 0.53\n",
      "  • Negative rate: 53.3%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now compute the race statistics\n",
    "print(\"Computing statistics by race\")\n",
    "\n",
    "race_stats = merged.groupby('RACE_SIMPLIFIED').agg({\n",
    "    'SUBJECT_ID': 'count',\n",
    "    'NEGATIVE_COMPLIANCE': ['sum', 'mean', 'std'],\n",
    "    'POSITIVE_COMPLIANCE': ['sum', 'mean', 'std'],\n",
    "    'NEUTRAL_TERMS': ['mean'],\n",
    "    'NEGATIVE_DENSITY': ['mean', 'std'],\n",
    "    'POSITIVE_DENSITY': ['mean', 'std'],\n",
    "    'HAS_PATTERNS': 'sum'\n",
    "}).round(3)\n",
    "\n",
    "race_stats.columns = ['_'.join(col).strip() for col in race_stats.columns.values]\n",
    "race_stats = race_stats.rename(columns={'SUBJECT_ID_count': 'NOTE_COUNT'})\n",
    "\n",
    "# Calculate negative rate percentage\n",
    "race_stats['NEGATIVE_RATE_%'] = (\n",
    "    race_stats['NEGATIVE_COMPLIANCE_sum'] / \n",
    "    (race_stats['NEGATIVE_COMPLIANCE_sum'] + race_stats['POSITIVE_COMPLIANCE_sum']) * 100\n",
    ").round(2)\n",
    "\n",
    "print(\"Statistics computed\")\n",
    "print()\n",
    "print(\"Summary Statistics by Race:\")\n",
    "print(race_stats[['NOTE_COUNT', 'NEGATIVE_COMPLIANCE_mean', \n",
    "                   'POSITIVE_COMPLIANCE_mean', 'NEGATIVE_RATE_%']])\n",
    "print()\n",
    "\n",
    "# Show some key findings\n",
    "print(\"KEY FINDINGS:\")\n",
    "for race in race_stats.index:\n",
    "    neg_mean = race_stats.loc[race, 'NEGATIVE_COMPLIANCE_mean']\n",
    "    pos_mean = race_stats.loc[race, 'POSITIVE_COMPLIANCE_mean']\n",
    "    neg_rate = race_stats.loc[race, 'NEGATIVE_RATE_%']\n",
    "    count = race_stats.loc[race, 'NOTE_COUNT']\n",
    "    \n",
    "    print(f\"{race}:\")\n",
    "    print(f\"  • Notes: {count:,}\")\n",
    "    print(f\"  • Avg negative terms per note: {neg_mean:.2f}\")\n",
    "    print(f\"  • Avg positive terms per note: {pos_mean:.2f}\")\n",
    "    print(f\"  • Negative rate: {neg_rate:.1f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d8fce50-dc66-436e-a756-9104ae63c8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating age (handling MIMIC-III date anonymization)...\n",
      " Age calculated\n",
      "\n",
      "Age statistics:\n",
      "count    56797.000000\n",
      "mean        58.152594\n",
      "std         22.845068\n",
      "min          0.000000\n",
      "25%         48.361396\n",
      "50%         62.904860\n",
      "75%         75.279945\n",
      "max         88.999316\n",
      "Name: AGE, dtype: float64\n",
      "\n",
      "Age distribution:\n",
      "AGE_GROUP\n",
      "Under 18     4346\n",
      "18-34        3551\n",
      "35-49        7525\n",
      "50-64       15398\n",
      "65+         25977\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Exporting spreadsheets...\n",
      "======================================================================\n",
      " Detailed records: mimic_detailed_records_20251103_101407.csv\n",
      " Race statistics: mimic_race_statistics_20251103_101407.csv\n",
      "Lexicon reference: mimic_lexicon_reference_20251103_101407.csv\n",
      " Cross-tabulation: mimic_race_admissiontype_crosstab_20251103_101407.csv\n",
      "\n",
      " ALL FILES EXPORTED SUCCESSFULLY!\n",
      "\n",
      "Total notes analyzed: 59,652\n",
      "Notes with negative compliance terms: 19,273\n",
      "Notes with positive compliance terms: 19,939\n",
      "Notes with unique patterns: 9,259\n",
      "\n",
      "Files created:\n",
      "  1. mimic_detailed_records_20251103_101407.csv\n",
      "  2. mimic_race_statistics_20251103_101407.csv\n",
      "  3. mimic_lexicon_reference_20251103_101407.csv\n",
      "  4. mimic_race_admissiontype_crosstab_20251103_101407.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate AGE more carefully to handle MIMIC-III's date anonymization\n",
    "print(\"Calculating age (handling MIMIC-III date anonymization)...\")\n",
    "\n",
    "# Convert to datetime\n",
    "merged['DOB'] = pd.to_datetime(merged['DOB'], errors='coerce')\n",
    "merged['ADMITTIME'] = pd.to_datetime(merged['ADMITTIME'], errors='coerce')\n",
    "\n",
    "# Calculate age safely\n",
    "def calculate_age_safe(row):\n",
    "    try:\n",
    "        if pd.isna(row['DOB']) or pd.isna(row['ADMITTIME']):\n",
    "            return np.nan\n",
    "        \n",
    "        age = (row['ADMITTIME'] - row['DOB']).days / 365.25\n",
    "        \n",
    "        # MIMIC-III shifts dates for patients >89 to year 2200+\n",
    "        # If age is negative or > 150, it's anonymized\n",
    "        if age < 0 or age > 150:\n",
    "            return 90  # Use 90 as proxy for elderly patients\n",
    "        elif age > 89:\n",
    "            return 90  # Cap at 90 as per MIMIC-III convention\n",
    "        else:\n",
    "            return age\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "merged['AGE'] = merged.apply(calculate_age_safe, axis=1)\n",
    "\n",
    "# Create age groups\n",
    "merged['AGE_GROUP'] = pd.cut(\n",
    "    merged['AGE'], \n",
    "    bins=[0, 18, 35, 50, 65, 100],\n",
    "    labels=['Under 18', '18-34', '35-49', '50-64', '65+'],\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "print(\" Age calculated\")\n",
    "print(f\"\\nAge statistics:\")\n",
    "print(merged['AGE'].describe())\n",
    "print(f\"\\nAge distribution:\")\n",
    "print(merged['AGE_GROUP'].value_counts().sort_index())\n",
    "print()\n",
    "\n",
    "# Now export with all columns\n",
    "print(\"Exporting spreadsheets...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Export 1: Detailed Records\n",
    "detailed_export = merged[[\n",
    "    'SUBJECT_ID', 'HADM_ID', 'AGE', 'AGE_GROUP', 'GENDER',\n",
    "    'ETHNICITY', 'RACE_SIMPLIFIED', 'ADMISSION_TYPE',\n",
    "    'TEXT_LENGTH', 'NEGATIVE_COMPLIANCE', 'POSITIVE_COMPLIANCE',\n",
    "    'NEUTRAL_TERMS', 'NEGATIVE_DENSITY', 'POSITIVE_DENSITY',\n",
    "    'PATTERNS', 'HAS_PATTERNS'\n",
    "]].copy()\n",
    "\n",
    "detailed_export['PATTERNS'] = detailed_export['PATTERNS'].apply(lambda x: ', '.join(x) if x else '')\n",
    "detailed_file = f\"mimic_detailed_records_{timestamp}.csv\"\n",
    "detailed_export.to_csv(detailed_file, index=False)\n",
    "print(f\" Detailed records: {detailed_file}\")\n",
    "\n",
    "# Export 2: Race Statistics\n",
    "stats_file = f\"mimic_race_statistics_{timestamp}.csv\"\n",
    "race_stats.to_csv(stats_file)\n",
    "print(f\" Race statistics: {stats_file}\")\n",
    "\n",
    "# Export 3: Lexicon Reference\n",
    "lexicon_df = pd.DataFrame([\n",
    "    {'Category': 'Negative Compliance', 'Term Count': 19, \n",
    "     'Examples': 'non-compliant, noncompliant, resistant, refuses, uncooperative, difficult, combative, aggressive, hostile, drug-seeking'},\n",
    "    {'Category': 'Positive Compliance', 'Term Count': 10, \n",
    "     'Examples': 'compliant, cooperative, pleasant, agreeable, appropriate'},\n",
    "    {'Category': 'Neutral Descriptors', 'Term Count': 7, \n",
    "     'Examples': 'alert, oriented, responsive, stable, calm'}\n",
    "])\n",
    "lexicon_file = f\"mimic_lexicon_reference_{timestamp}.csv\"\n",
    "lexicon_df.to_csv(lexicon_file, index=False)\n",
    "print(f\"Lexicon reference: {lexicon_file}\")\n",
    "\n",
    "# Export 4: Cross-tabulation (Race x Admission Type)\n",
    "crosstab = pd.crosstab(\n",
    "    merged['RACE_SIMPLIFIED'],\n",
    "    merged['ADMISSION_TYPE'],\n",
    "    values=merged['NEGATIVE_COMPLIANCE'],\n",
    "    aggfunc='mean'\n",
    ").round(2)\n",
    "crosstab_file = f\"mimic_race_admissiontype_crosstab_{timestamp}.csv\"\n",
    "crosstab.to_csv(crosstab_file)\n",
    "print(f\" Cross-tabulation: {crosstab_file}\")\n",
    "\n",
    "print()\n",
    "print(\" ALL FILES EXPORTED SUCCESSFULLY!\")\n",
    "print(f\"\\nTotal notes analyzed: {len(merged):,}\")\n",
    "print(f\"Notes with negative compliance terms: {(merged['NEGATIVE_COMPLIANCE'] > 0).sum():,}\")\n",
    "print(f\"Notes with positive compliance terms: {(merged['POSITIVE_COMPLIANCE'] > 0).sum():,}\")\n",
    "print(f\"Notes with unique patterns: {merged['HAS_PATTERNS'].sum():,}\")\n",
    "print()\n",
    "print(\"Files created:\")\n",
    "print(f\"  1. {detailed_file}\")\n",
    "print(f\"  2. {stats_file}\")\n",
    "print(f\"  3. {lexicon_file}\")\n",
    "print(f\"  4. {crosstab_file}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d0aa00c-629e-46d7-8f9c-48b83baef14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RACIAL BIAS ANALYSIS - KEY FINDINGS\n",
      "\n",
      "                 NOTE_COUNT  NEGATIVE_COMPLIANCE_sum  \\\n",
      "RACE_SIMPLIFIED                                        \n",
      "ASIAN                  1516                      769   \n",
      "BLACK                  5721                     4475   \n",
      "HISPANIC               2034                     1118   \n",
      "OTHER                  1717                      996   \n",
      "UNKNOWN                6409                     3251   \n",
      "WHITE                 42255                    25537   \n",
      "\n",
      "                 NEGATIVE_COMPLIANCE_mean  NEGATIVE_COMPLIANCE_std  \\\n",
      "RACE_SIMPLIFIED                                                      \n",
      "ASIAN                               0.507                    1.070   \n",
      "BLACK                               0.782                    1.338   \n",
      "HISPANIC                            0.550                    1.114   \n",
      "OTHER                               0.580                    1.174   \n",
      "UNKNOWN                             0.507                    1.071   \n",
      "WHITE                               0.604                    1.207   \n",
      "\n",
      "                 POSITIVE_COMPLIANCE_sum  POSITIVE_COMPLIANCE_mean  \\\n",
      "RACE_SIMPLIFIED                                                      \n",
      "ASIAN                                840                     0.554   \n",
      "BLACK                               3386                     0.592   \n",
      "HISPANIC                            1200                     0.590   \n",
      "OTHER                                901                     0.525   \n",
      "UNKNOWN                             2297                     0.358   \n",
      "WHITE                              22390                     0.530   \n",
      "\n",
      "                 POSITIVE_COMPLIANCE_std  NEUTRAL_TERMS_mean  \\\n",
      "RACE_SIMPLIFIED                                                \n",
      "ASIAN                              0.903               3.470   \n",
      "BLACK                              0.947               3.490   \n",
      "HISPANIC                           0.913               3.549   \n",
      "OTHER                              0.925               3.611   \n",
      "UNKNOWN                            0.717               3.071   \n",
      "WHITE                              0.894               3.441   \n",
      "\n",
      "                 NEGATIVE_DENSITY_mean  NEGATIVE_DENSITY_std  \\\n",
      "RACE_SIMPLIFIED                                                \n",
      "ASIAN                            0.304                 0.652   \n",
      "BLACK                            0.453                 0.815   \n",
      "HISPANIC                         0.340                 0.771   \n",
      "OTHER                            0.336                 0.687   \n",
      "UNKNOWN                          0.394                 0.902   \n",
      "WHITE                            0.384                 0.826   \n",
      "\n",
      "                 POSITIVE_DENSITY_mean  POSITIVE_DENSITY_std  \\\n",
      "RACE_SIMPLIFIED                                                \n",
      "ASIAN                            0.427                 0.770   \n",
      "BLACK                            0.388                 0.735   \n",
      "HISPANIC                         0.417                 0.727   \n",
      "OTHER                            0.368                 0.689   \n",
      "UNKNOWN                          0.285                 0.692   \n",
      "WHITE                            0.365                 0.715   \n",
      "\n",
      "                 HAS_PATTERNS_sum  NEGATIVE_RATE_%  \n",
      "RACE_SIMPLIFIED                                     \n",
      "ASIAN                         188            47.79  \n",
      "BLACK                        1051            56.93  \n",
      "HISPANIC                      339            48.23  \n",
      "OTHER                         235            52.50  \n",
      "UNKNOWN                       688            58.60  \n",
      "WHITE                        6758            53.28  \n",
      "\n",
      "COMPARATIVE ANALYSIS:\n",
      "\n",
      "Negative Compliance Language:\n",
      "  • BLACK patients: 0.782 terms/note\n",
      "  • WHITE patients: 0.604 terms/note\n",
      "  • Difference: +29.5%\n",
      "\n",
      "Positive Compliance Language:\n",
      "  • BLACK patients: 0.592 terms/note\n",
      "  • WHITE patients: 0.530 terms/note\n",
      "  • Difference: +11.7%\n",
      "\n",
      "\n",
      "Races Ranked by Negative Compliance (highest to lowest):\n",
      "  BLACK       : 0.782 terms/note  (n=5,721)\n",
      "  WHITE       : 0.604 terms/note  (n=42,255)\n",
      "  OTHER       : 0.580 terms/note  (n=1,717)\n",
      "  HISPANIC    : 0.550 terms/note  (n=2,034)\n",
      "  ASIAN       : 0.507 terms/note  (n=1,516)\n",
      "  UNKNOWN     : 0.507 terms/note  (n=6,409)\n"
     ]
    }
   ],
   "source": [
    "# Display the race statistics in detail\n",
    "print(\"RACIAL BIAS ANALYSIS - KEY FINDINGS\")\n",
    "print()\n",
    "print(race_stats)\n",
    "print()\n",
    "\n",
    "# Calculate some interesting comparisons\n",
    "print(\"COMPARATIVE ANALYSIS:\")\n",
    "\n",
    "# Compare BLACK vs WHITE\n",
    "if 'BLACK' in race_stats.index and 'WHITE' in race_stats.index:\n",
    "    black_neg = race_stats.loc['BLACK', 'NEGATIVE_COMPLIANCE_mean']\n",
    "    white_neg = race_stats.loc['WHITE', 'NEGATIVE_COMPLIANCE_mean']\n",
    "    diff_pct = ((black_neg - white_neg) / white_neg * 100)\n",
    "    \n",
    "    print(f\"\\nNegative Compliance Language:\")\n",
    "    print(f\"  • BLACK patients: {black_neg:.3f} terms/note\")\n",
    "    print(f\"  • WHITE patients: {white_neg:.3f} terms/note\")\n",
    "    print(f\"  • Difference: {diff_pct:+.1f}%\")\n",
    "    \n",
    "    black_pos = race_stats.loc['BLACK', 'POSITIVE_COMPLIANCE_mean']\n",
    "    white_pos = race_stats.loc['WHITE', 'POSITIVE_COMPLIANCE_mean']\n",
    "    diff_pos_pct = ((black_pos - white_pos) / white_pos * 100)\n",
    "    \n",
    "    print(f\"\\nPositive Compliance Language:\")\n",
    "    print(f\"  • BLACK patients: {black_pos:.3f} terms/note\")\n",
    "    print(f\"  • WHITE patients: {white_pos:.3f} terms/note\")\n",
    "    print(f\"  • Difference: {diff_pos_pct:+.1f}%\")\n",
    "\n",
    "# Show all races ranked by negative compliance\n",
    "print(f\"\\n\\nRaces Ranked by Negative Compliance (highest to lowest):\")\n",
    "ranked = race_stats.sort_values('NEGATIVE_COMPLIANCE_mean', ascending=False)\n",
    "for race in ranked.index:\n",
    "    neg_mean = ranked.loc[race, 'NEGATIVE_COMPLIANCE_mean']\n",
    "    count = ranked.loc[race, 'NOTE_COUNT']\n",
    "    print(f\"  {race:12s}: {neg_mean:.3f} terms/note  (n={count:,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "df5a40d9-787c-4d62-89fd-8cdbaab44b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICAL SIGNIFICANCE TESTS\n",
      "\n",
      "Chi-Square Test (Negative Compliance by Race):\n",
      "  χ² = 235.08, p-value = 8.70e-49\n",
      "  Result: SIGNIFICANT (p < 0.001)\n",
      "\n",
      "T-Test (BLACK vs WHITE - Negative Compliance):\n",
      "  t = 10.320, p-value = 6.07e-25\n",
      "  Result: SIGNIFICANT (p < 0.001)\n",
      "  Cohen's d = 0.145 (small effect)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(\"STATISTICAL SIGNIFICANCE TESTS\")\n",
    "\n",
    "# Chi-square test: Are negative compliance rates different by race?\n",
    "# Create contingency table\n",
    "contingency = pd.crosstab(\n",
    "    merged['RACE_SIMPLIFIED'],\n",
    "    merged['NEGATIVE_COMPLIANCE'] > 0\n",
    ")\n",
    "\n",
    "chi2, p_value, dof, expected = stats.chi2_contingency(contingency)\n",
    "print(f\"\\nChi-Square Test (Negative Compliance by Race):\")\n",
    "print(f\"  χ² = {chi2:.2f}, p-value = {p_value:.2e}\")\n",
    "print(f\"  Result: {'SIGNIFICANT' if p_value < 0.001 else 'Not significant'} (p < 0.001)\")\n",
    "\n",
    "# T-test: BLACK vs WHITE negative compliance\n",
    "black_notes = merged[merged['RACE_SIMPLIFIED'] == 'BLACK']['NEGATIVE_COMPLIANCE']\n",
    "white_notes = merged[merged['RACE_SIMPLIFIED'] == 'WHITE']['NEGATIVE_COMPLIANCE']\n",
    "\n",
    "t_stat, p_value_ttest = stats.ttest_ind(black_notes, white_notes)\n",
    "print(f\"\\nT-Test (BLACK vs WHITE - Negative Compliance):\")\n",
    "print(f\"  t = {t_stat:.3f}, p-value = {p_value_ttest:.2e}\")\n",
    "print(f\"  Result: {'SIGNIFICANT' if p_value_ttest < 0.001 else 'Not significant'} (p < 0.001)\")\n",
    "\n",
    "# Effect size (Cohen's d)\n",
    "cohens_d = (black_notes.mean() - white_notes.mean()) / np.sqrt(\n",
    "    ((len(black_notes)-1)*black_notes.std()**2 + (len(white_notes)-1)*white_notes.std()**2) / \n",
    "    (len(black_notes) + len(white_notes) - 2)\n",
    ")\n",
    "print(f\"  Cohen's d = {cohens_d:.3f} ({'small' if abs(cohens_d) < 0.5 else 'medium' if abs(cohens_d) < 0.8 else 'large'} effect)\")\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "43c7c7d0-8c0f-462e-81f6-cdef7e42fb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Exporting all results to one Excel workbook...\n",
      " All results exported to: mimic_analysis_results_20251103_114041.xlsx\n",
      "\n",
      "ANALYSIS COMPLETE!\n",
      "Next Steps:\n",
      " Open the Excel file and explore the tabs.\n",
      " Review race statistics for disparities.\n",
      " Examine cross-tabulations for patterns.\n",
      " Consider running statistical significance tests.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "print(\" Exporting all results to one Excel workbook...\")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "excel_file = f\"mimic_analysis_results_{timestamp}.xlsx\"\n",
    "\n",
    "# Prepare all dataframes\n",
    "detailed_export = merged[[\n",
    "    'SUBJECT_ID', 'HADM_ID', 'AGE', 'AGE_GROUP', 'GENDER',\n",
    "    'ETHNICITY', 'RACE_SIMPLIFIED', 'ADMISSION_TYPE',\n",
    "    'TEXT_LENGTH', 'NEGATIVE_COMPLIANCE', 'POSITIVE_COMPLIANCE',\n",
    "    'NEUTRAL_TERMS', 'NEGATIVE_DENSITY', 'POSITIVE_DENSITY',\n",
    "    'PATTERNS', 'HAS_PATTERNS'\n",
    "]].copy()\n",
    "\n",
    "# Convert list patterns to readable text\n",
    "detailed_export['PATTERNS'] = detailed_export['PATTERNS'].apply(lambda x: ', '.join(x) if isinstance(x, list) else '')\n",
    "\n",
    "lexicon_df = pd.DataFrame([\n",
    "    {'Category': 'Negative Compliance', 'Term Count': len(LEXICON['negative_compliance']), \n",
    "     'Examples': ', '.join(LEXICON['negative_compliance'][:10])},\n",
    "    {'Category': 'Positive Compliance', 'Term Count': len(LEXICON['positive_compliance']), \n",
    "     'Examples': ', '.join(LEXICON['positive_compliance'][:10])},\n",
    "    {'Category': 'Neutral Descriptors', 'Term Count': len(LEXICON['neutral_descriptors']), \n",
    "     'Examples': ', '.join(LEXICON['neutral_descriptors'][:10])}\n",
    "])\n",
    "\n",
    "crosstab = pd.crosstab(\n",
    "    merged['RACE_SIMPLIFIED'],\n",
    "    merged['ADMISSION_TYPE'],\n",
    "    values=merged['NEGATIVE_COMPLIANCE'],\n",
    "    aggfunc='mean'\n",
    ").round(2)\n",
    "\n",
    "# Save everything to one Excel workbook\n",
    "with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
    "    detailed_export.to_excel(writer, sheet_name='Detailed Records', index=False)\n",
    "    race_stats.to_excel(writer, sheet_name='Race Statistics')\n",
    "    lexicon_df.to_excel(writer, sheet_name='Lexicon Reference', index=False)\n",
    "    crosstab.to_excel(writer, sheet_name='Race x Admission Type')\n",
    "\n",
    "print(f\" All results exported to: {excel_file}\")\n",
    "\n",
    "print()\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"Next Steps:\")\n",
    "print(\" Open the Excel file and explore the tabs.\")\n",
    "print(\" Review race statistics for disparities.\")\n",
    "print(\" Examine cross-tabulations for patterns.\")\n",
    "print(\" Consider running statistical significance tests.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b9429932-223e-48f1-a4c6-b67462d96b4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Excel file created at: C:\\Users\\kehin\\word-bias\\scripts\\mimic_analysis_20251103_115048.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Assume 'merged' is your DataFrame containing all your analysis\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "excel_file_path = rf\"C:\\Users\\kehin\\word-bias\\scripts\\mimic_analysis_{timestamp}.xlsx\"\n",
    "\n",
    "# Export the full DataFrame to Excel\n",
    "merged.to_excel(excel_file_path, index=False, sheet_name='Analysis')\n",
    "\n",
    "print(\" Excel file created at:\", excel_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c1c4ccbd-beef-41c4-ab8f-2951919647bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting XlsxWriter\n",
      "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
      "Installing collected packages: XlsxWriter\n",
      "Successfully installed XlsxWriter-3.2.9\n",
      " Excel workbook created successfully at: C:\\Users\\kehin\\word-bias\\scripts\\mimic_analysis_20251103_120831.xlsx\n"
     ]
    }
   ],
   "source": [
    "!pip install XlsxWriter\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Timestamp for file naming\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "excel_file_path = rf\"C:\\Users\\kehin\\word-bias\\scripts\\mimic_analysis_{timestamp}.xlsx\"\n",
    "\n",
    "#  Sheet 1: Detailed Records \n",
    "detailed_export = merged.copy()\n",
    "# If PATTERNS column is a list, join them as string\n",
    "if 'PATTERNS' in detailed_export.columns:\n",
    "    detailed_export['PATTERNS'] = detailed_export['PATTERNS'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Sheet 2: Lexicon Reference \n",
    "lexicon_df = pd.DataFrame([\n",
    "    {'Category': 'Negative Compliance', 'Term Count': len(LEXICON['negative_compliance']),\n",
    "     'Examples': ', '.join(LEXICON['negative_compliance'][:10])},\n",
    "    {'Category': 'Positive Compliance', 'Term Count': len(LEXICON['positive_compliance']),\n",
    "     'Examples': ', '.join(LEXICON['positive_compliance'][:10])},\n",
    "    {'Category': 'Neutral Descriptors', 'Term Count': len(LEXICON['neutral_descriptors']),\n",
    "     'Examples': ', '.join(LEXICON['neutral_descriptors'][:10])}\n",
    "])\n",
    "\n",
    "# Optional Cross-tab (Race x Admission Type) \n",
    "crosstab = pd.crosstab(\n",
    "    merged['RACE_SIMPLIFIED'],\n",
    "    merged['ADMISSION_TYPE'],\n",
    "    values=merged['NEGATIVE_COMPLIANCE'],\n",
    "    aggfunc='mean'\n",
    ").round(2)\n",
    "\n",
    "# Export to Excel with multiple sheets\n",
    "with pd.ExcelWriter(excel_file_path, engine='xlsxwriter') as writer:\n",
    "    detailed_export.to_excel(writer, sheet_name='Detailed_Records', index=False)\n",
    "    lexicon_df.to_excel(writer, sheet_name='Lexicons', index=False)\n",
    "    crosstab.to_excel(writer, sheet_name='Race_Admission_Crosstab')\n",
    "\n",
    "print(\" Excel workbook created successfully at:\", excel_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e076840d-227c-43f1-9463-460e7d5f72d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique unknown terms: 984662\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Initialize a Counter\n",
    "unknown_counter = Counter()\n",
    "\n",
    "# Iterate row by row to avoid memory issues\n",
    "for terms in merged['UNKNOWN_TERMS']:\n",
    "    unknown_counter.update(terms)\n",
    "\n",
    "# Convert to DataFrame\n",
    "unknown_df = pd.DataFrame({\n",
    "    'Unknown_Term': list(unknown_counter.keys()),\n",
    "    'Frequency': list(unknown_counter.values())\n",
    "})\n",
    "\n",
    "print(f\"Total unique unknown terms: {len(unknown_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d61ae8a-8986-48d1-9018-ca4a0faa310b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
