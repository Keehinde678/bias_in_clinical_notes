{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d7007a-d277-4341-a2a1-c20d5f8e7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f2c02b0-f17c-4ace-815d-94be98b371d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MIMIC-III data files\n",
      " PATIENTS.csv: 46,520 patients\n",
      " ADMISSIONS.csv: 58,976 admissions\n",
      " NOTEEVENTS.csv: 2,083,180 clinical notes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load Dataset\n",
    "print(\"Loading MIMIC-III data files\")\n",
    "\n",
    "# Load the three main files\n",
    "patients = pd.read_csv(r\"C:\\Users\\kehin\\Downloads\\mimic-iii-extracted\\mimic-iii-clinical-database-1.4\\PATIENTS.csv.gz\")\n",
    "admissions = pd.read_csv(r\"C:\\Users\\kehin\\Downloads\\mimic-iii-extracted\\mimic-iii-clinical-database-1.4\\ADMISSIONS.csv.gz\")\n",
    "notes = pd.read_csv(r\"C:\\Users\\kehin\\Downloads\\mimic-iii-extracted\\mimic-iii-clinical-database-1.4\\NOTEEVENTS.csv.gz\")\n",
    "print(f\" PATIENTS.csv: {len(patients):,} patients\")\n",
    "print(f\" ADMISSIONS.csv: {len(admissions):,} admissions\")\n",
    "print(f\" NOTEEVENTS.csv: {len(notes):,} clinical notes\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "809b2b38-7ab4-4ab2-95f3-fbdb14132d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE CHARTTIME STORETIME  \\\n",
      "0     174       22532  167853.0  2151-08-04       NaN       NaN   \n",
      "1     175       13702  107527.0  2118-06-14       NaN       NaN   \n",
      "2     176       13702  167118.0  2119-05-25       NaN       NaN   \n",
      "3     177       13702  196489.0  2124-08-18       NaN       NaN   \n",
      "4     178       26880  135453.0  2162-03-25       NaN       NaN   \n",
      "\n",
      "            CATEGORY DESCRIPTION  CGID  ISERROR  \\\n",
      "0  Discharge summary      Report   NaN      NaN   \n",
      "1  Discharge summary      Report   NaN      NaN   \n",
      "2  Discharge summary      Report   NaN      NaN   \n",
      "3  Discharge summary      Report   NaN      NaN   \n",
      "4  Discharge summary      Report   NaN      NaN   \n",
      "\n",
      "                                                TEXT              ETHNICITY  \\\n",
      "0  Admission Date:  [**2151-7-16**]       Dischar...  UNKNOWN/NOT SPECIFIED   \n",
      "1  Admission Date:  [**2118-6-2**]       Discharg...                  WHITE   \n",
      "2  Admission Date:  [**2119-5-4**]              D...                  WHITE   \n",
      "3  Admission Date:  [**2124-7-21**]              ...                  WHITE   \n",
      "4  Admission Date:  [**2162-3-3**]              D...                  WHITE   \n",
      "\n",
      "  GENDER                  DOB  \n",
      "0      F  2064-08-20 00:00:00  \n",
      "1      F  2037-05-03 00:00:00  \n",
      "2      F  2037-05-03 00:00:00  \n",
      "3      F  2037-05-03 00:00:00  \n",
      "4      M  2080-01-04 00:00:00  \n",
      "   ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE CHARTTIME STORETIME  \\\n",
      "0     174       22532  167853.0  2151-08-04       NaN       NaN   \n",
      "1     175       13702  107527.0  2118-06-14       NaN       NaN   \n",
      "2     176       13702  167118.0  2119-05-25       NaN       NaN   \n",
      "3     177       13702  196489.0  2124-08-18       NaN       NaN   \n",
      "4     178       26880  135453.0  2162-03-25       NaN       NaN   \n",
      "\n",
      "            CATEGORY DESCRIPTION  CGID  ISERROR  \\\n",
      "0  Discharge summary      Report   NaN      NaN   \n",
      "1  Discharge summary      Report   NaN      NaN   \n",
      "2  Discharge summary      Report   NaN      NaN   \n",
      "3  Discharge summary      Report   NaN      NaN   \n",
      "4  Discharge summary      Report   NaN      NaN   \n",
      "\n",
      "                                                TEXT GENDER  \\\n",
      "0  Admission Date:  [**2151-7-16**]       Dischar...      F   \n",
      "1  Admission Date:  [**2118-6-2**]       Discharg...      F   \n",
      "2  Admission Date:  [**2119-5-4**]              D...      F   \n",
      "3  Admission Date:  [**2124-7-21**]              ...      F   \n",
      "4  Admission Date:  [**2162-3-3**]              D...      M   \n",
      "\n",
      "                   DOB  neg_count  pos_count  \n",
      "0  2064-08-20 00:00:00          0          0  \n",
      "1  2037-05-03 00:00:00          0          0  \n",
      "2  2037-05-03 00:00:00          0          1  \n",
      "3  2037-05-03 00:00:00          0          0  \n",
      "4  2080-01-04 00:00:00          0          1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "patients = pd.read_csv(r\"C:\\Users\\kehin\\Downloads\\mimic-iii-extracted\\mimic-iii-clinical-database-1.4\\PATIENTS.csv.gz\")\n",
    "notes = pd.read_csv(r\"C:\\Users\\kehin\\Downloads\\mimic-iii-extracted\\mimic-iii-clinical-database-1.4\\NOTEEVENTS.csv.gz\")\n",
    "admissions = pd.read_csv(r\"C:\\Users\\kehin\\Downloads\\mimic-iii-extracted\\mimic-iii-clinical-database-1.4\\ADMISSIONS.csv.gz\")\n",
    "\n",
    "# Keep relevant columns\n",
    "patients = patients[['SUBJECT_ID', 'GENDER', 'DOB']]\n",
    "admissions = admissions[['SUBJECT_ID', 'HADM_ID', 'ETHNICITY']]\n",
    "\n",
    "# Merge patients → admissions\n",
    "patient_adm = pd.merge(admissions, patients, on='SUBJECT_ID', how='left')\n",
    "\n",
    "# Merge notes → patient + admission info\n",
    "notes_merged = pd.merge(notes, patient_adm, on=['SUBJECT_ID', 'HADM_ID'], how='left')\n",
    "\n",
    "print(notes_merged.head())\n",
    "\n",
    "chunk_size = 10000  # adjust based on memory\n",
    "filename =r\"C:\\Users\\kehin\\Downloads\\mimic-iii-extracted\\mimic-iii-clinical-database-1.4\\NOTEEVENTS.csv.gz\"\n",
    "  # path to your note events file\n",
    "\n",
    "def process_notes_chunk(chunk, patients_df):\n",
    "    # Merge chunk with patient demographics\n",
    "    merged_chunk = chunk.merge(patients_df, on=\"SUBJECT_ID\", how=\"left\")\n",
    "    \n",
    "    # Example: count negative compliance terms\n",
    "    neg_compliance = [\"non-compliant\", \"refuses\", \"uncooperative\"]\n",
    "    merged_chunk['neg_count'] = merged_chunk['TEXT'].str.lower().str.count(r'\\b(?:' + '|'.join(neg_compliance) + r')\\b')\n",
    "    \n",
    "    # Example: count positive compliance terms\n",
    "    pos_compliance = [\"cooperative\", \"adherent\", \"compliant\"]\n",
    "    merged_chunk['pos_count'] = merged_chunk['TEXT'].str.lower().str.count(r'\\b(?:' + '|'.join(pos_compliance) + r')\\b')\n",
    "    \n",
    "    return merged_chunk\n",
    "all_notes = []  # store processed chunks\n",
    "\n",
    "for chunk in pd.read_csv(filename, chunksize=chunk_size):\n",
    "    processed_chunk = process_notes_chunk(chunk, patients)\n",
    "    all_notes.append(processed_chunk)\n",
    "\n",
    "# Combine all chunks into a single DataFrame\n",
    "full_notes = pd.concat(all_notes, ignore_index=True)\n",
    "\n",
    "print(full_notes.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62e62517-7943-44b5-83d0-2948408688b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename_notes = r\"C:\\Users\\kehin\\Downloads\\mimic-iii-extracted\\mimic-iii-clinical-database-1.4\\NOTEEVENTS.csv.gz\"\n",
    "filename_adm = r\"C:\\Users\\kehin\\Downloads\\mimic-iii-extracted\\mimic-iii-clinical-database-1.4\\ADMISSIONS.csv.gz\"\n",
    "\n",
    "usecols_notes = ['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'TEXT']\n",
    "usecols_adm = ['SUBJECT_ID', 'HADM_ID', 'ETHNICITY']\n",
    "\n",
    "chunk_size = 1000  # adjust down if still too large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3190bc3-8e2b-425d-938b-1b48389d958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load admissions\n",
    "admissions = pd.read_csv(filename_adm, usecols=usecols_adm)\n",
    "\n",
    "# Standardize ethnicity\n",
    "def standardize_ethnicity(x):\n",
    "    x = str(x).upper()\n",
    "    if \"BLACK\" in x: return \"BLACK\"\n",
    "    elif \"WHITE\" in x: return \"WHITE\"\n",
    "    elif \"ASIAN\" in x: return \"ASIAN\"\n",
    "    elif \"HISPANIC\" in x: return \"HISPANIC\"\n",
    "    elif \"UNKNOWN\" in x or x.strip() == \"\": return \"UNKNOWN\"\n",
    "    else: return \"OTHER\"\n",
    "\n",
    "admissions['ETHNICITY'] = admissions['ETHNICITY'].apply(standardize_ethnicity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ad09fce-2e72-474e-8188-53e8ac3d0d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_compliance = [\"non-compliant\", \"refuses\", \"uncooperative\"]\n",
    "pos_compliance = [\"cooperative\", \"adherent\", \"compliant\"]\n",
    "content_words = [\"diagnosis\", \"procedure\", \"symptom\", \"medication\"]\n",
    "style_words = [\"pleasant\", \"difficult\", \"helpful\", \"agitated\"]\n",
    "framing_markers = [\"claims\", \"reports\", \"denies\", \"states\"]\n",
    "stereotype_words = [\"stoic\", \"hysterical\", \"competent\", \"warm\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96754800-c635-454f-bc0e-9229e15c0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk, adm_df):\n",
    "    # Merge with admissions\n",
    "    chunk = pd.merge(chunk, adm_df, on=['SUBJECT_ID', 'HADM_ID'], how='left')\n",
    "    \n",
    "    # Lowercase text\n",
    "    chunk['TEXT'] = chunk['TEXT'].astype(str).str.lower()\n",
    "    \n",
    "    # H1: negative/positive compliance\n",
    "    chunk['neg_count'] = chunk['TEXT'].str.count('|'.join(neg_compliance))\n",
    "    chunk['pos_count'] = chunk['TEXT'].str.count('|'.join(pos_compliance))\n",
    "    \n",
    "    # H2: content vs style words\n",
    "    chunk['content_count'] = chunk['TEXT'].str.count('|'.join(content_words))\n",
    "    chunk['style_count'] = chunk['TEXT'].str.count('|'.join(style_words))\n",
    "    \n",
    "    # H3: framing markers\n",
    "    chunk['framing_count'] = chunk['TEXT'].str.count('|'.join(framing_markers))\n",
    "    \n",
    "    # H4: stereotype descriptors\n",
    "    chunk['stereotype_count'] = chunk['TEXT'].str.count('|'.join(stereotype_words))\n",
    "    \n",
    "    return chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c9677e4-8517-4a2b-b166-4a90a971c4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIAN: Notes=84, Neg=0, Pos=4\n",
      "BLACK: Notes=215, Neg=5, Pos=19\n",
      "HISPANIC: Notes=122, Neg=2, Pos=12\n",
      "OTHER: Notes=138, Neg=1, Pos=7\n",
      "UNKNOWN: Notes=303, Neg=1, Pos=13\n",
      "WHITE: Notes=2138, Neg=27, Pos=175\n"
     ]
    }
   ],
   "source": [
    "# Lexicons\n",
    "neg_compliance = [\"non-compliant\", \"refuses\", \"uncooperative\"]\n",
    "pos_compliance = [\"cooperative\", \"adherent\", \"compliant\"]\n",
    "\n",
    "summary_dict = {}\n",
    "\n",
    "# Example: just process the first few chunks to get some preliminary results\n",
    "for i, chunk in enumerate(chunk_iter):\n",
    "    if i > 2:  # only process first 3 chunks for now\n",
    "        break\n",
    "    processed = process_chunk(chunk, admissions)\n",
    "    \n",
    "    # Count negative/positive terms\n",
    "    processed['neg_count'] = processed['TEXT'].apply(\n",
    "        lambda x: sum(1 for word in neg_compliance if word in x.lower())\n",
    "    )\n",
    "    processed['pos_count'] = processed['TEXT'].apply(\n",
    "        lambda x: sum(1 for word in pos_compliance if word in x.lower())\n",
    "    )\n",
    "\n",
    "    # Group by ethnicity and summarize\n",
    "    for eth, group in processed.groupby('ETHNICITY'):\n",
    "        if eth not in summary_dict:\n",
    "            summary_dict[eth] = {'notes_count': 0, 'neg_count': 0, 'pos_count': 0}\n",
    "        summary_dict[eth]['notes_count'] += len(group)\n",
    "        summary_dict[eth]['neg_count'] += group['neg_count'].sum()\n",
    "        summary_dict[eth]['pos_count'] += group['pos_count'].sum()\n",
    "\n",
    "# Quick view of results\n",
    "for eth, stats in summary_dict.items():\n",
    "    print(f\"{eth}: Notes={stats['notes_count']}, Neg={stats['neg_count']}, Pos={stats['pos_count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78db8146-e52f-4f5e-ab81-bf29f8f06f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ETHNICITY  notes_count   avg_neg   avg_pos  avg_content  avg_style  \\\n",
      "0     ASIAN        55253  0.001484  0.012868     0.662715   0.109551   \n",
      "1     BLACK       180416  0.005986  0.029232     0.750249   0.160014   \n",
      "2  HISPANIC        65219  0.003159  0.023306     0.782456   0.133182   \n",
      "3     OTHER       109628  0.002855  0.019420     0.771454   0.133351   \n",
      "4   UNKNOWN       144843  0.002893  0.026429     0.432171   0.126668   \n",
      "5     WHITE      1295985  0.004432  0.027324     0.823405   0.162248   \n",
      "\n",
      "   avg_framing  avg_stereotype  \n",
      "0     0.116464        0.113007  \n",
      "1     0.198408        0.101726  \n",
      "2     0.176743        0.107576  \n",
      "3     0.127659        0.117187  \n",
      "4     0.102608        0.078844  \n",
      "5     0.173565        0.109361  \n"
     ]
    }
   ],
   "source": [
    "summary = notes_processed.groupby('ETHNICITY').agg(\n",
    "    notes_count=('ROW_ID', 'count'),\n",
    "    avg_neg=('neg_count', 'mean'),\n",
    "    avg_pos=('pos_count', 'mean'),\n",
    "    avg_content=('content_count', 'mean'),\n",
    "    avg_style=('style_count', 'mean'),\n",
    "    avg_framing=('framing_count', 'mean'),\n",
    "    avg_stereotype=('stereotype_count', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a7679-bdd0-4b38-8ea0-e883f10d4517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
